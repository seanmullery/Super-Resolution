{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Super-Resolution\n",
    "Thank you to Jeremy Howard and Rachel Thomas of fast.ai for the code on which this program is based.\n",
    "\n",
    "It uses the approach of Justin Johnson, Alexandre Alahi, Li Fei-Fei in the following paper.(https://arxiv.org/abs/1603.08155)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import importlib\n",
    "import utils2; importlib.reload(utils2)\n",
    "from utils2 import *\n",
    "\n",
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "from scipy.misc import imsave\n",
    "from keras import metrics\n",
    "\n",
    "from vgg16_avg import VGG16_Avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tell Tensorflow to use no more GPU RAM than necessary\n",
    "#limit_mem()\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.8\n",
    "session = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnmem= 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = './'\n",
    "dpath = 'data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using VGG16. Therefore, we need to subtract the mean of each channel of the imagenet data and reverse the order of RGB->BGR since those are the preprocessing steps that the VGG authors did - so their model won't work unless we do the same thing.\n",
    "We can do this in one step using broadcasting, which is a topic we'll be returning to many times during this course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(os.path.join(path, 'CampusBuild7272.jpg'))\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rn_mean = np.array([123.68, 116.779, 103.939], dtype=np.float32)\n",
    "preproc = lambda x: (x - rn_mean)[:, :, :, ::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we generate images from this network, we'll need to undo the above preprocessing in order to view them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "deproc = lambda x,s: np.clip(x.reshape(s)[:, :, :, ::-1] + rn_mean, 0, 255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recreate input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = VGG16_Avg(include_top=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use content loss to create a super-resolution network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following data will need to be downloaded from http://files.fast.ai/data/ into your dpath directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "arr_lr = bcolz.open(dpath+'trn_resized_72.bc')[:] #this changes a bcolz array into an numpy array by slicing\n",
    "arr_hr = bcolz.open(dpath+'trn_resized_288.bc')[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parms = {'verbose': 0, 'callbacks': [TQDMNotebookCallback(leave_inner=True)]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start we'll define some of the building blocks of our network. In particular recall the residual block (as used in Resnet), which is just a sequence of 2 convolutional layers that is added to the initial block input. We also have a de-convolutional layer (also known as a \"transposed convolution\" or \"fractionally strided convolution\"), whose purpose is to learn to \"undo\" the convolutional function. It does this by padding the smaller image in such a way to apply filters on it to produce a larger image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_block(x, numFilters, size, stride=(2,2), mode='same', act=True):\n",
    "    x = Conv2D(numFilters, size, strides=stride, padding=mode)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    return Activation('relu')(x) if act else x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def res_block(ip, numFilters=64):\n",
    "    x = conv_block(ip, numFilters, 3, (1,1))\n",
    "    x = conv_block(x, numFilters, 3, (1,1), act=False)\n",
    "    return add([x, ip])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def deconv_block(x, numFilters, size, shape, stride=(2,2)):\n",
    "    x = Conv2DTranspose(numFilters, (size, size), strides=stride, \n",
    "        padding='same')(x) #, output_shape=(None,)+shape\n",
    "    x = BatchNormalization()(x)\n",
    "    return Activation('relu')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def up_block(x, numFilters, size):\n",
    "    x = keras.layers.UpSampling2D()(x)\n",
    "    x = Conv2D(numFilters, (size, size), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    return Activation('relu')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def up_block(x, numFilters, size):\n",
    "    x = keras.layers.UpSampling2D()(x)\n",
    "    x = Conv2D(numFilters, (size, size), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    return Activation('relu')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### this is a simple clipping layer which is some of our experiments replaces the tanh.\n",
    "def clipLayer(x):    \n",
    "    return keras.backend.clip(x, -1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model here is using the previously defined blocks to encode a low resolution image and then upsample it to match the same image in high resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "\n",
    "inp=Input(arr_lr.shape[1:])\n",
    "conv_in=conv_block(inp, 64, 9, (1,1))\n",
    "x=res_block(conv_in)\n",
    "for i in range(3): x=res_block(x)\n",
    "x=up_block(x, 64, 3)\n",
    "x=up_block(x, 64, 3)\n",
    "#x=Conv2D(3, (9, 9), activation='tanh', padding='same')(x)\n",
    "\n",
    "\n",
    "x = layers.Conv2D(3, 9, padding='same', kernel_initializer='random_normal',\n",
    "                bias_initializer='zeros')(x)\n",
    "\n",
    "### uncomment/comment the following lines for the combination of BN, clipping, tanh you wish to test.\n",
    "#x = layers.BatchNormalization()(x)\n",
    "#x=layers.Lambda(clipLayer, tuple(list((288,288,3))))(x)\n",
    "x = Activation('tanh')(x)\n",
    "outp=Lambda(lambda x: (x+1)*127.5)(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea here is we're going to feed two images to Vgg16 and compare their convolutional outputs at some layer. These two images are the target image (which in our case is the same as the original but at higher resolution), and the output of the previous network we just defined, which we hope will learn to output a high resolution image.\n",
    "The key then is to train this other network to produce an image that minimizes the loss between the outputs of some convolutional layer in Vgg16 (which the paper refers to as \"perceptual loss\"). In doing so, we are able to train a network that can upsample an image and recreate the higher resolution details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shp = (288, 288, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vgg_inp=Input(shp)\n",
    "vgg= VGG16(include_top=False, input_tensor=Lambda(preproc)(vgg_inp)) #Lambda, turns the function into a layer of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we only want to learn the \"upsampling network\", and are just using VGG to calculate the loss function, we set the Vgg layers to not be trainable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for l in vgg.layers: l.trainable=False "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important difference in training for super resolution is the loss function. We use what's known as a perceptual loss function (which is simply the content loss for some layer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it seems here we are taking content (perceptual output) from Layers 1,2 and 3 with only the fist convolutional layer in each. Remember that layers lower in the network maintian content detail better than upper layers like 4 and 5.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_outp(m, ln): return m.get_layer('block'+ str(ln)+'_conv1').output\n",
    "\n",
    "vgg_content = Model(vgg_inp, [get_outp(vgg, o) for o in [1,2,3,4,5]]) #I think vgg_content is just the three layers. but we can treat\n",
    "                                                        #these like a network and the result of them by sending in an input\n",
    "vgg1 = vgg_content(vgg_inp) #Send in the high res target \n",
    "vgg2 = vgg_content(outp) # send in the generated high res prediction, note this is the output of our other model that we do train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean_sqr_b(diff): \n",
    "    print(\"mean_sqr\")\n",
    "    dims = list(range(1,K.ndim(diff)))\n",
    "    return K.expand_dims(K.sqrt(K.mean(diff**2, dims)), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "w=[1.0/10, 8.0/10, 1.0/10,0,0] #Taking 80% of the content or layer 2 but only 10% of content of layers 1 and 3\n",
    "def content_fn(x): \n",
    "    print(\"content_fn\")\n",
    "    res = 0; n=len(w)\n",
    "    for i in range(n): \n",
    "        res += mean_sqr_b(x[i]-x[i+n]) * w[i]\n",
    "        print(res, i)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg1+vgg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vgg1+vgg2 is not a sum it's just a concatenation of the two sets of output layers\n",
    "\n",
    "m_sr = Model([inp, vgg_inp], Lambda(content_fn)(vgg1+vgg2))\n",
    "targ = np.zeros((arr_hr.shape[0], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "m_sr.summary() ###Super-resolution network summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing the weights of the final BN Layer\n",
    "If you wish to set the initial weights of the final BatchNorm layer you can do so by uncommenting the line below.\n",
    "I have initialized to the RGB standard-deviation and RGB mean of the target distribution.\n",
    "final two lists are the running-mean and running-SD which we set to 0 and 1 respectively\n",
    "Keras BatchNorm implementation is here for reference\n",
    "https://github.com/keras-team/keras/blob/master/keras/layers/normalization.py\n",
    "if you change the architecture you will have to determine the correct layer number \n",
    "by using generator.summary() you can count your way down to the appropriate BN layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#m_sr.layers[37].set_weights(np.array([[0.606, 0.585, 0.595],[-0.142, -0.1866, -0.27282], [0,0,0], [1,1,1]]))\n",
    "\n",
    "### you may wish to freeze the weights of the final BN Layer. If so you can do that with the following line.\n",
    "#generator.layers[37].trainable = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A note on the values used above\n",
    "\n",
    "In Jeremy's data, images are resized to a square aspect ratio without changing the original aspect ratio of the images. This means than the bottom portion of the images are colour values of [0,0,0]. We have choosen to calculate the values only of the data set only using the valid colour values. Using all values darkened the images in the initial part of training seemed to required the network to learn its way back to appropriate values.\n",
    "The [0,0,0] appear to start from rows 215."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(arr_hr[:,0:215,:,0].mean()/127.5-1)\n",
    "print(arr_hr[:,0:215,:,1].mean()/127.5-1)\n",
    "print(arr_hr[:,0:215,:,2].mean()/127.5-1)\n",
    "print(math.sqrt(arr_hr[:,0:215,:,0].var())/127.5)\n",
    "print(math.sqrt(arr_hr[:,0:215,:,1].var())/127.5)\n",
    "print(math.sqrt(arr_hr[:,0:215,:,2].var())/127.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we compile this chain of models and we can pass it the original low resolution image as well as the high resolution to train on. We also define a zero vector as a target parameter, which is a necessary parameter when calling fit on a keras model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "from time import sleep\n",
    "from tqdm import trange\n",
    "start = 0\n",
    "batch_size = 8\n",
    "m_sr.compile('adam', 'mse')\n",
    "targ = np.zeros((batch_size, 1))\n",
    "targ.shape\n",
    "\n",
    "\n",
    "iterations = 2001\n",
    "t =  trange(iterations, desc='')\n",
    "top_model = Model(inp, outp) #This is the generator only and we can use this to test inference.\n",
    "\n",
    "current_step = 0\n",
    "img=Image.open((os.path.join(path, 'CampusBuild7272.jpg')))\n",
    "img_arr = np.expand_dims(np.array(img), 0)\n",
    "K.set_value(m_sr.optimizer.lr, 1e-4)\n",
    "loss_curve = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "t =  trange(iterations, desc='')\n",
    "for i in t:\n",
    "    stop = start + batch_size\n",
    "    \n",
    "    lr_batch = arr_lr[start: stop]\n",
    "    hr_batch = arr_hr[start: stop]\n",
    "    \n",
    "    loss = m_sr.train_on_batch([lr_batch, hr_batch], targ)\n",
    "    \n",
    "    start += batch_size\n",
    "    \n",
    "    if start > len(arr_hr[0]) - batch_size:\n",
    "            start = 0   \n",
    "    \n",
    "    t.set_description('%d Loss ' % (loss))\n",
    "    if current_step%50 == 0:\n",
    "        p = top_model.predict(img_arr)\n",
    "    \n",
    "        imsave(os.path.join(path, 'out/sr_' + str(current_step)+'.png'), p[0])\n",
    "    loss_curve.append(loss)\n",
    "    current_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "with open(os.path.join(path, 'out/loss.csv'), 'w') as myfile:\n",
    "            wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "            wr.writerow(loss_curve)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
